{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T12:52:49.853980Z",
     "start_time": "2018-04-04T12:52:49.847995Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "import time\n",
    "from sys import stdout\n",
    "import cPickle as pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T12:52:53.793382Z",
     "start_time": "2018-04-04T12:52:53.786016Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T12:52:53.817857Z",
     "start_time": "2018-04-04T12:52:53.807857Z"
    }
   },
   "outputs": [],
   "source": [
    "corpora_directory = os.path.join(\"\", \"corpora\")\n",
    "\n",
    "hansard_transcript_directory = os.path.join(corpora_directory, \"hansard-transcripts\")\n",
    "hansard_speeches_directory = os.path.join(corpora_directory, \"hansard-speeches\")\n",
    "\n",
    "saves_directory = os.path.join(\"\", \"saves/hansard\")\n",
    "\n",
    "figures_directory = os.path.join(\"\", os.path.join(\"figures\", \"hansard\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T12:52:53.837289Z",
     "start_time": "2018-04-04T12:52:53.831396Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_create_directory(directory):\n",
    "    \"\"\"Checks if directory exists. If not, create it.\"\"\"\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T12:52:53.864646Z",
     "start_time": "2018-04-04T12:52:53.857902Z"
    }
   },
   "outputs": [],
   "source": [
    "check_create_directory(saves_directory)\n",
    "\n",
    "check_create_directory(hansard_speeches_directory)\n",
    "\n",
    "check_create_directory(figures_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get speeches from hansard transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T21:20:07.603989Z",
     "start_time": "2018-03-11T21:20:07.542644Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_speeches(soup):\n",
    "    \"\"\"Get the speeches of a transcript in html markup.\n",
    "\n",
    "    This function:\n",
    "        (a) finds where the speakers are using the heuristics of looking for bold tags in the form of <b>\n",
    "        and <strong>, though they are also used for other texts in the transcript (e.g. absent, present, etc.)\n",
    "        (b) assumes all text after the current bold tag and the next bold tag belong to the same speech\n",
    "        (c) these speeches are in the form of a dictionary where the key is the speaker, and the val is a list of the\n",
    "        speech in paragraphs, each paragraph is an item in the list.\n",
    "\n",
    "    Parameter:\n",
    "        BeautifulSoup object (BeautifulSoup(html file, \"html.parser\"))\n",
    "\n",
    "    Return:\n",
    "        list of dictionaries where {key: value} pairs are {speaker: speech} pairs, where speech is\n",
    "        a list of paragraphs in the speech.\n",
    "    \"\"\"\n",
    "\n",
    "    # get ptags and bold/strong tag locationsa\n",
    "    c = 0\n",
    "    btag_location = list()\n",
    "    ptags = list()\n",
    "    #     strongs = list()\n",
    "\n",
    "    for ptag in soup.find_all(\"p\"):\n",
    "        ptags.append(ptag)\n",
    "\n",
    "        if (ptag.b is not None) or (ptag.strong is not None):\n",
    "            btag_location.append(c)\n",
    "\n",
    "        c += 1\n",
    "\n",
    "    # use btag locations and ptags list to get speeches\n",
    "    speeches = list()  # container for dictionary of speeches\n",
    "    c = 0\n",
    "\n",
    "    for start_index in btag_location:\n",
    "        # key is MP, and val is a list of the paragraphs of the speech\n",
    "        speech = dict()\n",
    "\n",
    "        # get end_index\n",
    "        if c == len(btag_location) - 1:\n",
    "            end_index = len(ptags)\n",
    "        else:\n",
    "            end_index = btag_location[c + 1]\n",
    "\n",
    "        # get tags and content\n",
    "        try:\n",
    "            key = ptags[start_index].strong.string.strip(\"\\n\")\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                key = ptags[start_index].b.string.strip(\"\\n\")\n",
    "            except:\n",
    "                try:\n",
    "                    key = ptags[start_index].b.string\n",
    "                except:\n",
    "                    key = \"key\"\n",
    "\n",
    "        val = ptags[start_index].contents[2].string.strip(\"\\n\")\n",
    "\n",
    "        if len(ptags[start_index].contents) > 3:\n",
    "            for i in range(3, len(ptags[start_index].contents) + 1):\n",
    "                try:\n",
    "                    if ptags[start_index].contents[i] == \"\\n\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            val += ptags[start_index].contents[i].string.strip(\"\\n\")\n",
    "                        except:\n",
    "                            pass\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "        #  initiate dictionary with the first value\n",
    "        speech[key] = [val]\n",
    "\n",
    "        # get rest of speech using start and end index\n",
    "        for i in range(start_index + 1, end_index):\n",
    "            for content in ptags[i].contents:\n",
    "                if content.string != None:\n",
    "                    try:\n",
    "                        val = content.string.strip(\"\\n\")\n",
    "                    except:\n",
    "                        print(content.string)\n",
    "\n",
    "                speech[key].append(val)\n",
    "\n",
    "        speeches.append(speech)\n",
    "\n",
    "        c += 1\n",
    "\n",
    "    return speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T21:20:08.741612Z",
     "start_time": "2018-03-11T21:20:08.723122Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_numbered_filename(counter, directory=hansard_speeches_directory):\n",
    "    new_name = str(counter) + \".txt\"\n",
    "    new_filepath = os.path.join(directory, new_name)\n",
    "    return new_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T21:20:09.049026Z",
     "start_time": "2018-03-11T21:20:09.039620Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_procedural_speech(speech, char_limit=150, word_limit=30):\n",
    "    \"\"\"Tag a speech as a 'procedural' speech using a crude hueristic as follows.\n",
    "\n",
    "    Mark a speech as 'procedural' if:\n",
    "    (i)  length of speech is less than 100, or\n",
    "    (ii) number of words in speech is less than 20.\n",
    "\n",
    "    Examples are:\n",
    "        (i)   Madam Speaker, I agree (consent, concur, etc.)\n",
    "        (ii)  Does the Leader of the House have the general assent of the hon Members present to so move?\n",
    "        (iii) Order. Leader of the House.\n",
    "        (iv)  Mr Speaker, may I take Question Nos 2 and 3 together?\n",
    "        (v)   Mr Saktiandi, keep it short, please.\n",
    "        (vi)  Yes, Mr Seah Kian Peng.\n",
    "\n",
    "    Argument:\n",
    "        speech is a string/unicode variable\n",
    "\n",
    "    Return:\n",
    "        Boolean: True if procedural\n",
    "    \"\"\"\n",
    "\n",
    "    # remove empty lines and empty spaces\n",
    "    speech = speech.replace(r\"\\n\", \"\")\n",
    "    re.sub(r\"\\s+\", \" \", speech)\n",
    "\n",
    "    if (len(speech) <= char_limit) or (len(speech.split(\" \")) <= word_limit):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get speeches of hansard transcripts\n",
    "Helper functions:\n",
    "   1. get_speeches(soup)\n",
    "   2. check_procedural_speech(text)\n",
    " \n",
    "Steps:\n",
    "1. get all transcript file names from directory\n",
    "2. pass transcript (html format) to bs to get soup object\n",
    "3. use get_speeches(soup) to obtain speaker-speech chunks \n",
    "3. write speech to a running ordered number text file if check_procedural_speech(speech) is False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T15:16:47.067185Z",
     "start_time": "2018-03-09T15:14:23.252686Z"
    }
   },
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    transcript_filenames = [f for f in os.listdir(hansard_transcript_directory)]\n",
    "    transcript_filenames.sort()\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for filename in transcript_filenames:\n",
    "        stdout.write(\"\\rProcessing %s\" % filename)\n",
    "\n",
    "        # get content of filename\n",
    "        filepath = os.path.join(hansard_transcript_directory, filename)\n",
    "        with codecs.open(filepath, \"r\", encoding=\"utf-8\") as fh:\n",
    "            transcript = \" \".join(fh.readlines())\n",
    "\n",
    "        # convert to bs object\n",
    "        soup = BeautifulSoup(transcript, \"html.parser\")\n",
    "\n",
    "        # get speeches\n",
    "        speeches = get_speeches(soup)\n",
    "\n",
    "        # unpack each speech into its own file\n",
    "        for speech_dict in speeches:\n",
    "            for speaker, speech in speech_dict.items():\n",
    "                # check if speech is 'procedural', if yes then pass\n",
    "                if check_procedural_speech(\" \".join(speech)):\n",
    "                    pass\n",
    "                else:  # write speech to file\n",
    "                    counter += 1  # increase counter at speech level\n",
    "\n",
    "                    with codecs.open(\n",
    "                        new_numbered_filename(counter), \"w\", encoding=\"utf-8\"\n",
    "                    ) as f:\n",
    "                        # add speaker to first line\n",
    "                        try:\n",
    "                            f.write(speaker)\n",
    "                        except TypeError:\n",
    "                            f.write(\"\\n\")\n",
    "\n",
    "                        # add speech\n",
    "                        for paragraph in speech:\n",
    "                            f.write(paragraph)\n",
    "                            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sentences and train gensim Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T21:20:13.821743Z",
     "start_time": "2018-03-11T21:20:13.792958Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_speech(speech):\n",
    "    \"\"\"Clean speeches from parliament transcript.\n",
    "\n",
    "    Removes:\n",
    "        (i)   Timestamps\n",
    "        (ii)  column markers\n",
    "        (iii) page markers\n",
    "        (iv)  remove strings in parentheses and brackets\n",
    "        (v)   strang non-english characters\n",
    "        (vi)  digits\n",
    "        (vii) whitespaces\n",
    "\n",
    "    Argument:\n",
    "        speech is a unicode/str variable (needs unicode for further spacy processing)\n",
    "\n",
    "    Return:\n",
    "        unicode/str var\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # remove strange non-eng characters\n",
    "    speech = \"\".join([char for char in speech if char in string.printable])\n",
    "\n",
    "    # remove transcript markers\n",
    "    pattern1 = \"page:\\s*?[0-9]+\"  # page makers e.g. page: xx\n",
    "    pattern2 = \"[0-9]+[.:]\\s*?[0-9]+\\s*?pm\"  # timestamp e.g. 5.55 pm\n",
    "    pattern3 = \"[0-9]+[.:]\\s*?[0-9]+\\s*?am\"  # timestamp e.g. 11:00 am\n",
    "    pattern4 = \"column:\\s*?[0-9]+\"  # column makers e.g. column: xxxx\n",
    "    pattern5 = \"\\(.+\\)\"  # remove anything in parentheses\n",
    "    pattern6 = \"\\[.+\\]\"  # remove anything in brackets\n",
    "\n",
    "    pattern = \"|\".join((pattern1, pattern2, pattern3, pattern4, pattern5, pattern6))\n",
    "    speech = re.sub(pattern, \" \", speech)\n",
    "\n",
    "    # remove digits\n",
    "    speech = re.sub(r\"[0-9]+\", \" \", speech)\n",
    "\n",
    "    # remove whitespaces\n",
    "    speech = re.sub(\"\\s+\", \" \", speech)\n",
    "\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T21:20:15.769785Z",
     "start_time": "2018-03-11T21:20:15.747576Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_token(token):\n",
    "    \"\"\"\n",
    "    Checks if a token (of type spacy.tokens.token.Token) meets certain exclusion criteria using spacy.\n",
    "    If yes, then return false, return true otherwise.\n",
    "    Exclusion criteria are:\n",
    "        (i)   entity types of person, date, time, etc.\n",
    "        (ii)  punctuation\n",
    "        (iii) stopword\n",
    "    Entity types found here: https://spacy.io/usage/linguistic-features\n",
    "\n",
    "    Argument:\n",
    "        spacy.tokens.token.Token object.\n",
    "\n",
    "    Return:\n",
    "        Boolean.\n",
    "    \"\"\"\n",
    "    entity_types = [\n",
    "        \"PERSON\",\n",
    "        \"DATE\",\n",
    "        \"TIME\",\n",
    "        \"PERCENT\",\n",
    "        \"MONEY\",\n",
    "        \"QUANTITY\",\n",
    "        \"ORDINAL\",\n",
    "        \"CARDINAL\",\n",
    "    ]\n",
    "\n",
    "    if token.ent_type_ in entity_types:\n",
    "        return False\n",
    "\n",
    "    if token.is_punct:\n",
    "        return False\n",
    "\n",
    "    if token.is_stop:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T15:20:03.519336Z",
     "start_time": "2018-03-09T15:20:03.490835Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_stream(directory=hansard_speeches_directory):\n",
    "    \"\"\"\n",
    "    Generator: iterate over all documents in the directory (=hansard_speeches_directory),\n",
    "    and yield a sentence at a time (=list of utf8 tokens):\n",
    "\n",
    "        (i)   get document from each file\n",
    "        (ii)  clean document content using clean_speech()\n",
    "        (iii) process using spacy's nlp\n",
    "        (iv)  store lemmatized token to list if not punct and not stopword\n",
    "        (v)   yield list of sentence tokens\n",
    "\n",
    "    Argument:\n",
    "        Directory containing document files.\n",
    "\n",
    "    Return:\n",
    "        List of sentence tokens for each sentence.\n",
    "\n",
    "    \"\"\"\n",
    "    # get speech from directory\n",
    "    for file_no in range(1, len(os.listdir(hansard_speeches_directory)) + 1):\n",
    "        #     for file_no in range(1, 50):\n",
    "        filepath = os.path.join(directory, str(file_no) + \".txt\")\n",
    "        with codecs.open(filepath, \"r\", encoding=\"utf-8\") as fh:\n",
    "            speech = \" \".join(fh.readlines())\n",
    "\n",
    "        # basic cleaning of speech\n",
    "        speech = clean_speech(speech)\n",
    "\n",
    "        # get sentences\n",
    "        parsed_speech = nlp(speech, disable=[\"tagger\"])\n",
    "\n",
    "        for (\n",
    "            sentence\n",
    "        ) in (\n",
    "            parsed_speech.sents\n",
    "        ):  # append lemmatised token to list if token meets conditions (see return_token())\n",
    "            sentence_tokens = [\n",
    "                token.lemma_.lower() for token in sentence if return_token(token)\n",
    "            ]\n",
    "\n",
    "            yield sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T13:42:15.530364Z",
     "start_time": "2018-03-09T13:42:15.476971Z"
    }
   },
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_speech.ents):\n",
    "    print 'Entity {}:'.format(num + 1), entity, '-', entity.label_\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T15:18:52.964165Z",
     "start_time": "2018-03-09T15:18:50.366160Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = list(sentence_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T15:18:57.835411Z",
     "start_time": "2018-03-09T15:18:57.562918Z"
    }
   },
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T21:37:21.673471Z",
     "start_time": "2018-03-09T17:17:28.023041Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(\n",
    "    sentences=sentence_stream(),\n",
    "    min_count=10,\n",
    "    threshold=0.55,\n",
    "    max_vocab_size=40000000,\n",
    "    delimiter=\"_\",\n",
    "    scoring=\"npmi\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T21:45:11.993560Z",
     "start_time": "2018-03-09T21:44:37.424711Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(bigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T21:48:29.619261Z",
     "start_time": "2018-03-09T21:48:29.415581Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent in ss:\n",
    "    print \" \".join(bigram_phraser[sent])\n",
    "    print \"---------------------------\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T21:48:02.243149Z",
     "start_time": "2018-03-09T21:48:02.157907Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_phraser_directory = os.path.join(saves_directory, \"bigram-phraser-hansard\")\n",
    "bigram_phraser.save(bigram_phraser_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T21:37:14.927448Z",
     "start_time": "2018-03-11T21:37:14.874825Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_phraser_directory = os.path.join(saves_directory, \"bigram-phraser-hansard\")\n",
    "bigram_phraser = Phraser.load(bigram_phraser_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-08T19:26:14.028862Z",
     "start_time": "2018-03-08T10:37:38.777Z"
    }
   },
   "source": [
    "## Trigram phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:07:32.617176Z",
     "start_time": "2018-03-09T21:48:45.437930Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(\n",
    "    sentences=bigram_phraser[sentence_stream()],\n",
    "    min_count=10,\n",
    "    threshold=0.7,\n",
    "    max_vocab_size=40000000,\n",
    "    delimiter=\"_\",\n",
    "    scoring=\"npmi\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:09:15.979571Z",
     "start_time": "2018-03-09T23:08:05.073683Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(trigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:09:19.939645Z",
     "start_time": "2018-03-09T23:09:19.566050Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent in bigram_phraser[ss]:\n",
    "    print \" \".join(trigram_phraser[sent])\n",
    "    print \"---------------------------\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T23:11:30.315815Z",
     "start_time": "2018-03-09T23:11:30.210798Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_phraser_directory = os.path.join(saves_directory, \"trigram-phraser-hansard\")\n",
    "trigram_phraser.save(trigram_phraser_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T21:37:17.489692Z",
     "start_time": "2018-03-11T21:37:17.438213Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_phraser_directory = os.path.join(saves_directory, \"trigram-phraser-hansard\")\n",
    "trigram_phraser = Phraser.load(trigram_phraser_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T21:52:27.932591Z",
     "start_time": "2018-03-11T21:52:27.905826Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenized_corpus_stream(directory=hansard_speeches_directory):\n",
    "    # get speech from directory\n",
    "    trigram_corpus = list()\n",
    "\n",
    "    for ix, speech_file in enumerate(os.listdir(directory)):\n",
    "        stdout.write(\"\\rProcessing %s/%s\" % (ix + 1, len(os.listdir(directory))))\n",
    "\n",
    "        filepath = os.path.join(directory, speech_file)\n",
    "        with codecs.open(filepath, \"r\", encoding=\"utf-8\") as fh:\n",
    "            speech = \" \".join(fh.readlines())\n",
    "\n",
    "        # basic cleaning of speech\n",
    "        speech = clean_speech(speech)\n",
    "\n",
    "        # tokenized speech using nlp\n",
    "        tokenized_speech = [\n",
    "            token.lemma_.lower()\n",
    "            for token in nlp(speech, disable=[\"tagger\"])\n",
    "            if return_token(token)\n",
    "        ]\n",
    "\n",
    "        # convert bigrams using bigram_phraser\n",
    "        bigram_speech = bigram_phraser[tokenized_speech]\n",
    "\n",
    "        # convert trigrams using trigram_phraser\n",
    "        trigram_speech = trigram_phraser[bigram_speech]\n",
    "\n",
    "        trigram_corpus.append(trigram_speech)\n",
    "\n",
    "    return trigram_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T23:24:24.961853Z",
     "start_time": "2018-03-11T21:52:29.067276Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_corpus = tokenized_corpus_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T23:25:41.903210Z",
     "start_time": "2018-03-11T23:25:33.549030Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_corpus_directory = os.path.join(saves_directory, \"trigram-corpus-hansard\")\n",
    "\n",
    "with open(trigram_corpus_directory, \"wb\") as f:\n",
    "    pickle.dump(trigram_corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T01:27:06.985008Z",
     "start_time": "2018-03-10T01:26:46.614240Z"
    }
   },
   "outputs": [],
   "source": [
    "# load corpus\n",
    "trigram_corpus_directory = os.path.join(saves_directory, \"trigram-corpus-hansard\")\n",
    "with open(trigram_corpus_directory, \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T23:26:10.787987Z",
     "start_time": "2018-03-11T23:26:02.872096Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(trigram_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T23:26:28.733749Z",
     "start_time": "2018-03-11T23:26:28.381222Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T23:26:32.164403Z",
     "start_time": "2018-03-11T23:26:32.133164Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary_directory = os.path.join(saves_directory, \"trigram-dictionary-hansard.dict\")\n",
    "dictionary.save(dictionary_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized tokenized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T23:26:44.712286Z",
     "start_time": "2018-03-11T23:26:35.860671Z"
    }
   },
   "outputs": [],
   "source": [
    "# load corpus\n",
    "trigram_corpus_directory = os.path.join(saves_directory, \"trigram-corpus-hansard\")\n",
    "with open(trigram_corpus_directory, \"rb\") as f:\n",
    "    trigram_corpus = pickle.load(f)\n",
    "\n",
    "# load dictionary\n",
    "dictionary_directory = os.path.join(saves_directory, \"trigram-dictionary-hansard.dict\")\n",
    "dictionary = Dictionary.load(dictionary_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T23:27:10.115545Z",
     "start_time": "2018-03-11T23:26:45.649997Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorized_corpus_directory = os.path.join(\n",
    "    saves_directory, \"trigram-vectorized-corpus-hansard.mm\"\n",
    ")\n",
    "MmCorpus.serialize(\n",
    "    vectorized_corpus_directory, [dictionary.doc2bow(doc) for doc in trigram_corpus]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized_corpus = [dictionary.doc2bow(doc) in tokenized_corpus_stream()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized_corpus_directory = os.path.join(saves_directory, 'trigram-vectorized-corpus-hansard.mm')\n",
    "# MmCorpus.serialize(vectorized_corpus_directory, vectorized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find optimal k\n",
    "Requires:\n",
    "* vectorized_corpus\n",
    "* dictionary\n",
    "* corpus (in text form - list of lists of document tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T14:51:45.900339Z",
     "start_time": "2018-03-30T14:51:36.355044Z"
    }
   },
   "outputs": [],
   "source": [
    "# load vectorized_corpus as stream\n",
    "vectorized_corpus_directory = os.path.join(\n",
    "    saves_directory, \"trigram-vectorized-corpus-hansard.mm\"\n",
    ")\n",
    "vectorized_corpus = MmCorpus(vectorized_corpus_directory)\n",
    "\n",
    "# load dictionary\n",
    "dictionary_directory = os.path.join(saves_directory, \"trigram-dictionary-hansard.dict\")\n",
    "dictionary = Dictionary.load(dictionary_directory)\n",
    "\n",
    "# load corpus\n",
    "trigram_corpus_directory = os.path.join(saves_directory, \"trigram-corpus-hansard\")\n",
    "with open(trigram_corpus_directory, \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T14:51:46.844215Z",
     "start_time": "2018-03-30T14:51:46.837985Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"c_v\", \"c_uci\", \"c_npmi\", \"u_mass\", \"num_topics\"]).set_index(\n",
    "    \"num_topics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:11:06.486371Z",
     "start_time": "2018-03-30T14:51:49.298492Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_TOPICS = 150\n",
    "TOPN = 10  # top n words in topics to use when evaluating topic coherence\n",
    "PROCESSES = 1  # I think this is how cpu cores to use when estimating coherence\n",
    "\n",
    "for k in np.arange(2, MAX_TOPICS, 2):\n",
    "    ALPHA = float(50) / k\n",
    "    stdout.write(\n",
    "        \"\\rTopic modelling %s topics (%s)\" % (k, time.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "    )\n",
    "\n",
    "    # train base LDA model\n",
    "    tm = LdaMulticore(\n",
    "        corpus=vectorized_corpus,\n",
    "        num_topics=k,\n",
    "        id2word=dictionary,\n",
    "        workers=2,\n",
    "        chunksize=2000,\n",
    "        passes=1,\n",
    "        batch=False,\n",
    "        alpha=ALPHA,\n",
    "        eta=None,\n",
    "        decay=0.5,\n",
    "        offset=1.0,\n",
    "        eval_every=10,\n",
    "        iterations=100,\n",
    "        gamma_threshold=0.001,\n",
    "        random_state=0,\n",
    "        minimum_probability=0.01,\n",
    "        minimum_phi_value=0.01,\n",
    "        per_word_topics=False,\n",
    "    )\n",
    "\n",
    "    # Train coherence models\n",
    "    c_v_model = CoherenceModel(\n",
    "        model=tm,\n",
    "        topics=None,\n",
    "        texts=corpus,\n",
    "        corpus=vectorized_corpus,\n",
    "        dictionary=None,\n",
    "        coherence=\"c_v\",\n",
    "        topn=TOPN,\n",
    "        processes=PROCESSES,\n",
    "    )\n",
    "\n",
    "    c_uci_model = CoherenceModel(\n",
    "        model=tm,\n",
    "        topics=None,\n",
    "        texts=corpus,\n",
    "        corpus=vectorized_corpus,\n",
    "        dictionary=None,\n",
    "        coherence=\"c_uci\",\n",
    "        topn=TOPN,\n",
    "        processes=PROCESSES,\n",
    "    )\n",
    "\n",
    "    c_npmi_model = CoherenceModel(\n",
    "        model=tm,\n",
    "        topics=None,\n",
    "        texts=corpus,\n",
    "        corpus=vectorized_corpus,\n",
    "        dictionary=None,\n",
    "        coherence=\"c_npmi\",\n",
    "        topn=TOPN,\n",
    "        processes=PROCESSES,\n",
    "    )\n",
    "\n",
    "    u_mass_model = CoherenceModel(\n",
    "        model=tm,\n",
    "        topics=None,\n",
    "        texts=corpus,\n",
    "        corpus=vectorized_corpus,\n",
    "        dictionary=None,\n",
    "        coherence=\"u_mass\",\n",
    "        topn=TOPN,\n",
    "        processes=PROCESSES,\n",
    "    )\n",
    "\n",
    "    # store coherence scores\n",
    "    df.set_value(k, \"c_v\", c_v_model.get_coherence())\n",
    "    df.set_value(k, \"c_uci\", c_uci_model.get_coherence())\n",
    "    df.set_value(k, \"c_npmi\", c_npmi_model.get_coherence())\n",
    "    df.set_value(k, \"u_mass\", u_mass_model.get_coherence())\n",
    "    df.to_excel(\"coherence-scores-hansard-alpha.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:11:21.597219Z",
     "start_time": "2018-03-31T10:11:21.537237Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"coherence-scores-hansard-alpha.xlsx\", index_col=\"num_topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:11:26.116504Z",
     "start_time": "2018-03-31T10:11:26.054161Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:11:33.069272Z",
     "start_time": "2018-03-31T10:11:33.060869Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.apply(lambda c: pd.to_numeric(c, errors=\"coerce\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:11:37.075155Z",
     "start_time": "2018-03-31T10:11:37.060524Z"
    }
   },
   "outputs": [],
   "source": [
    "glob_params = {\n",
    "    \"legend.fontsize\": \"xx-large\",\n",
    "    \"figure.titlesize\": \"xx-large\",\n",
    "    \"axes.labelsize\": \"xx-large\",\n",
    "    \"axes.titlesize\": \"xx-large\",\n",
    "    \"xtick.labelsize\": \"xx-large\",\n",
    "    \"ytick.labelsize\": \"xx-large\",\n",
    "    \"lines.markersize\": 12.0,\n",
    "    \"figure.figsize\": [12, 8],\n",
    "}\n",
    "\n",
    "plt.rcParams.update(glob_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:11:40.458334Z",
     "start_time": "2018-03-31T10:11:40.434882Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"c_v\"].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:11:43.896296Z",
     "start_time": "2018-03-31T10:11:43.878991Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"c_v\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:11:48.679330Z",
     "start_time": "2018-03-31T10:11:47.181083Z"
    }
   },
   "outputs": [],
   "source": [
    "c_v_directory = os.path.join(figures_directory, \"c-v\")\n",
    "\n",
    "local_max = (df[\"c_v\"].argmax(), df[\"c_v\"].max())\n",
    "\n",
    "plt.plot(df.index, df[\"c_v\"])\n",
    "\n",
    "plt.plot(local_max[0], local_max[1], \"o\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# https://matplotlib.org/users/annotations.html\n",
    "plt.annotate(\n",
    "    \"Local max\\n (\" + str(local_max[0]) + \", \" + str(local_max[1])[:5] + \")\",\n",
    "    xy=(local_max),\n",
    "    xytext=(local_max[0] + 5, local_max[1] - 0.005),\n",
    "    size=\"x-large\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"c_v coherence measure\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(c_v_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot c_uci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:12:11.357361Z",
     "start_time": "2018-03-31T10:12:11.349175Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"c_uci\"].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:12:14.533166Z",
     "start_time": "2018-03-31T10:12:14.516666Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"c_uci\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:12:19.095248Z",
     "start_time": "2018-03-31T10:12:18.015239Z"
    }
   },
   "outputs": [],
   "source": [
    "c_uci_directory = os.path.join(figures_directory, \"c-uci\")\n",
    "\n",
    "local_max = (df[\"c_uci\"].argmax(), df[\"c_uci\"].max())\n",
    "\n",
    "plt.plot(df.index, df[\"c_uci\"])\n",
    "\n",
    "plt.plot(local_max[0], local_max[1], \"o\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# https://matplotlib.org/users/annotations.html\n",
    "plt.annotate(\n",
    "    \"Local max\\n (\" + str(local_max[0]) + \", \" + str(local_max[1])[:5] + \")\",\n",
    "    xy=(local_max),\n",
    "    xytext=(local_max[0] + 5, local_max[1] - 0.01),\n",
    "    size=\"x-large\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"c_uci coherence measure\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(c_uci_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot c_npmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:12:35.013530Z",
     "start_time": "2018-03-31T10:12:35.004329Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"c_npmi\"].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:12:38.380380Z",
     "start_time": "2018-03-31T10:12:38.362257Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"c_npmi\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:13:08.501800Z",
     "start_time": "2018-03-31T10:13:06.515751Z"
    }
   },
   "outputs": [],
   "source": [
    "c_npmi_directory = os.path.join(figures_directory, \"c-npmi\")\n",
    "\n",
    "local_max = (df[\"c_npmi\"].argmax(), df[\"c_npmi\"].max())\n",
    "\n",
    "plt.plot(df.index, df[\"c_npmi\"])\n",
    "\n",
    "plt.plot(local_max[0], local_max[1], \"o\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# https://matplotlib.org/users/annotations.html\n",
    "plt.annotate(\n",
    "    \"Local max\\n (\" + str(local_max[0]) + \", \" + str(local_max[1])[:5] + \")\",\n",
    "    xy=(local_max),\n",
    "    xytext=(local_max[0] + 5, local_max[1] - 0.001),\n",
    "    size=\"x-large\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"c_npmi coherence measure\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(c_npmi_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot u_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:13:16.325001Z",
     "start_time": "2018-03-31T10:13:16.305197Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"u_mass\"].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:13:20.262435Z",
     "start_time": "2018-03-31T10:13:20.249099Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"u_mass\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:13:25.035483Z",
     "start_time": "2018-03-31T10:13:23.756623Z"
    }
   },
   "outputs": [],
   "source": [
    "u_mass_directory = os.path.join(figures_directory, \"u-mass\")\n",
    "\n",
    "local_max = (df[\"u_mass\"].argmax(), df[\"u_mass\"].max())\n",
    "\n",
    "plt.plot(df.index, df[\"u_mass\"])\n",
    "\n",
    "plt.plot(local_max[0], local_max[1], \"o\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# https://matplotlib.org/users/annotations.html\n",
    "plt.annotate(\n",
    "    \"local max\\n (\" + str(local_max[0]) + \", \" + str(local_max[1])[:5] + \")\",\n",
    "    xy=(local_max),\n",
    "    xytext=(round(local_max[0], -1), round(local_max[1], 1)),\n",
    "    arrowprops=dict(facecolor=\"black\"),\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"u_mass coherence measure\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(u_mass_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot normalised scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:13:29.252176Z",
     "start_time": "2018-03-31T10:13:29.234738Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalising coherence scores\n",
    "def normalise_scores(vector):\n",
    "    \"\"\"\n",
    "    Normalise scores to (0, 1) range using:\n",
    "        x_normalised = (x - x_min)/(x_max - x_min)\n",
    "\n",
    "    Argument:\n",
    "        List, array, series type\n",
    "\n",
    "    Returns:\n",
    "        Transformed list, array, series\n",
    "    \"\"\"\n",
    "\n",
    "    # cach min and max\n",
    "    min_x = vector.min()\n",
    "    range_x = vector.max() - min_x\n",
    "\n",
    "    transformed_vector = [(x - min_x) / range_x for x in vector]\n",
    "\n",
    "    return transformed_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:13:32.752345Z",
     "start_time": "2018-03-31T10:13:32.742106Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_scores_df = df.apply(normalise_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T17:08:37.835789Z",
     "start_time": "2018-03-11T17:08:37.763566Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-31T10:13:39.101425Z",
     "start_time": "2018-03-31T10:13:38.335766Z"
    }
   },
   "outputs": [],
   "source": [
    "normalised_directory = os.path.join(figures_directory, \"normalised-scores\")\n",
    "\n",
    "local_max = (\n",
    "    transformed_scores_df[\"c_npmi\"].argmax(),\n",
    "    transformed_scores_df[\"c_npmi\"].max(),\n",
    ")\n",
    "\n",
    "# create plot\n",
    "plt.plot(transformed_scores_df.index, transformed_scores_df[\"c_v\"], color=\"teal\")\n",
    "plt.plot(transformed_scores_df.index, transformed_scores_df[\"c_uci\"], color=\"orange\")\n",
    "plt.plot(transformed_scores_df.index, transformed_scores_df[\"c_npmi\"], color=\"maroon\")\n",
    "\n",
    "plt.plot(local_max[0], local_max[1], \"o\", color=\"red\", alpha=0.5)\n",
    "\n",
    "plt.annotate(\n",
    "    \"Local max\\n (\" + str(local_max[0]) + \", \" + str(local_max[1])[:5] + \")\",\n",
    "    xy=(local_max),\n",
    "    xytext=(local_max[0] + 5, local_max[1] - 0.03),\n",
    "    size=\"x-large\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Normalised coherence scores [0, 1]\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# plt.savefig(normalised_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-12T19:22:34.999742Z",
     "start_time": "2018-03-12T19:22:34.983636Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_scores_df.nlargest(5, \"c_v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n",
    "Requires:\n",
    "* vectorized_corpus\n",
    "* dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T12:53:04.578656Z",
     "start_time": "2018-04-04T12:53:04.556033Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary_directory = os.path.join(saves_directory, \"trigram-dictionary-hansard.dict\")\n",
    "dictionary = Dictionary.load(dictionary_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T12:53:05.926212Z",
     "start_time": "2018-04-04T12:53:05.904924Z"
    }
   },
   "outputs": [],
   "source": [
    "# from gensim.test.utils import datapath\n",
    "\n",
    "vectorized_corpus_directory = os.path.join(\n",
    "    saves_directory, \"trigram-vectorized-corpus-hansard.mm\"\n",
    ")\n",
    "\n",
    "vectorized_corpus = MmCorpus(vectorized_corpus_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T18:58:42.433216Z",
     "start_time": "2018-04-04T18:58:42.415569Z"
    }
   },
   "outputs": [],
   "source": [
    "K = 80\n",
    "ALPHA = float(50) / K\n",
    "ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T19:53:08.952792Z",
     "start_time": "2018-04-04T18:58:45.884392Z"
    }
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(\n",
    "    corpus=vectorized_corpus,\n",
    "    num_topics=K,\n",
    "    id2word=dictionary,\n",
    "    chunksize=2000,\n",
    "    passes=1,\n",
    "    workers=1,\n",
    "    alpha=ALPHA,\n",
    "    eta=None,\n",
    "    decay=0.5,\n",
    "    offset=1.0,\n",
    "    eval_every=None,\n",
    "    iterations=1000,\n",
    "    gamma_threshold=0.001,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=0,\n",
    "    minimum_phi_value=0.01,\n",
    "    per_word_topics=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T19:55:57.629513Z",
     "start_time": "2018-04-04T19:55:57.569939Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_directory = os.path.join(saves_directory, \"lda-hansard-80-alpha.model\")\n",
    "lda.save(lda_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 50 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T14:20:50.356372Z",
     "start_time": "2018-04-04T14:20:50.349898Z"
    }
   },
   "outputs": [],
   "source": [
    "K = 50\n",
    "ALPHA = float(50) / K\n",
    "ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T14:29:44.239599Z",
     "start_time": "2018-04-04T14:21:03.895684Z"
    }
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(\n",
    "    corpus=vectorized_corpus,\n",
    "    num_topics=K,\n",
    "    id2word=dictionary,\n",
    "    chunksize=2000,\n",
    "    passes=1,\n",
    "    workers=1,\n",
    "    alpha=ALPHA,\n",
    "    eta=None,\n",
    "    decay=0.5,\n",
    "    offset=1.0,\n",
    "    eval_every=None,\n",
    "    iterations=1000,\n",
    "    gamma_threshold=0.001,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=0,\n",
    "    minimum_phi_value=0.01,\n",
    "    per_word_topics=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T14:30:22.315424Z",
     "start_time": "2018-04-04T14:30:22.205346Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_directory = os.path.join(saves_directory, \"lda-hansard-50-alpha.model\")\n",
    "lda.save(lda_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K = 100 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T14:30:24.141609Z",
     "start_time": "2018-04-04T14:30:24.124367Z"
    }
   },
   "outputs": [],
   "source": [
    "K = 100\n",
    "ALPHA = float(50) / K\n",
    "ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T15:30:37.706073Z",
     "start_time": "2018-04-04T14:30:26.309236Z"
    }
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(\n",
    "    corpus=vectorized_corpus,\n",
    "    num_topics=K,\n",
    "    id2word=dictionary,\n",
    "    chunksize=2000,\n",
    "    passes=1,\n",
    "    workers=1,\n",
    "    alpha=ALPHA,\n",
    "    eta=None,\n",
    "    decay=0.5,\n",
    "    offset=1.0,\n",
    "    eval_every=None,\n",
    "    iterations=1000,\n",
    "    gamma_threshold=0.001,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=0,\n",
    "    minimum_phi_value=0.01,\n",
    "    per_word_topics=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-04T15:30:37.761130Z",
     "start_time": "2018-04-04T15:30:37.719324Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_directory = os.path.join(saves_directory, \"lda-hansard-100-alpha.model\")\n",
    "lda.save(lda_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_media)",
   "language": "python",
   "name": "venv_media"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "671px",
    "left": "128px",
    "top": "120.84px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "636px",
    "left": "13.8305px",
    "right": "20px",
    "top": "474.984px",
    "width": "173px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
