{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T04:53:29.784754Z",
     "start_time": "2018-06-16T04:53:29.780329Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Standard library\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "from sys import stdout\n",
    "\n",
    "# Third-party - data/numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Third-party - NLP\n",
    "import spacy\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "# Notebook settings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T04:54:13.088655Z",
     "start_time": "2018-06-16T04:54:13.079899Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T04:54:17.355847Z",
     "start_time": "2018-06-16T04:54:17.350511Z"
    }
   },
   "outputs": [],
   "source": [
    "corpora_directory = os.path.join(\"\", \"corpora\")\n",
    "\n",
    "saves_directory = os.path.join(\"\", \"saves\", \"st\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T04:54:20.302274Z",
     "start_time": "2018-06-16T04:54:20.294947Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_create_directory(directory):\n",
    "    \"\"\"Checks if directory exists. If not, create it.\"\"\"\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-16T04:54:20.873389Z",
     "start_time": "2018-06-16T04:54:20.868167Z"
    }
   },
   "outputs": [],
   "source": [
    "check_create_directory(saves_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ST corpus and clean corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:31:09.448995Z",
     "start_time": "2018-03-09T22:30:53.436464Z"
    }
   },
   "outputs": [],
   "source": [
    "st_file = os.path.join(corpora_directory, \"all_articles_final.xlsx\")\n",
    "\n",
    "df = pd.read_excel(st_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:35:24.423606Z",
     "start_time": "2018-03-09T22:33:36.650973Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_content(text):\n",
    "    #     # remove strange non-eng characters\n",
    "    #     text = \"\".join([unicode(char) for char in text if char in string.printable])\n",
    "\n",
    "    remove_list = [r\"\\n',\", r\"'\\n',\", r'\\n\",', r'\\n\"', r\"\\n\", r\"\\r\", \"$\", r\"\\\\\", \"\\\\\"]\n",
    "\n",
    "    for item in remove_list:\n",
    "        text = text.replace(item, \"\")\n",
    "\n",
    "    # remove digits\n",
    "    text = re.sub(r\"[0-9]+\", \"\", text)\n",
    "\n",
    "    # remove email addresses\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "\n",
    "    # replace multiple whitespactes with one whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "corpus_st = [\n",
    "    clean_content(df.get_value(row, \"content\")[1:-1]) for row, _ in df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T22:36:13.197966Z",
     "start_time": "2018-03-09T22:36:11.964077Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_st_directory = os.path.join(saves_directory, \"corpus-st\")\n",
    "\n",
    "with open(corpus_st_directory, \"wb\") as fh:\n",
    "    pickle.dump(corpus_st, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding custom stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T10:18:34.448911Z",
     "start_time": "2018-03-07T10:18:34.426493Z"
    }
   },
   "outputs": [],
   "source": [
    "additional_stopwords = [\"i\", \"mr\", \"dr\", \"ms\", \"tell\", \"cent\", \"reporter\"]\n",
    "\n",
    "# convert from str to unicode (spacy reads only unicode)\n",
    "additional_stopwords = [unicode(word) for word in additional_stopwords]\n",
    "\n",
    "# add to spacy's STOP_WORDS\n",
    "for word in additional_stopwords:\n",
    "    STOP_WORDS.add(word)\n",
    "\n",
    "# set word.is_stop to True for removal of stopwords\n",
    "for stopword in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting sentences and training phrasers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T01:56:41.509506Z",
     "start_time": "2018-03-10T01:56:32.039962Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_st_directory = os.path.join(saves_directory, \"corpus-st\")\n",
    "\n",
    "with open(corpus_st_directory, \"rb\") as fh:\n",
    "    corpus = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T01:56:45.713319Z",
     "start_time": "2018-03-10T01:56:45.698896Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_token(token):\n",
    "    \"\"\"\n",
    "    Checks if a token (of type spacy.tokens.token.Token) meets certain exclusion criteria using spacy.\n",
    "    If yes, then return false, return true otherwise.\n",
    "    Exclusion criteria are:\n",
    "        (i)   entity types of person, date, time, etc.\n",
    "        (ii)  punctuation\n",
    "        (iii) stopword\n",
    "    Entity types found here: https://spacy.io/usage/linguistic-features\n",
    "\n",
    "    Argument:\n",
    "        spacy.tokens.token.Token object.\n",
    "\n",
    "    Return:\n",
    "        Boolean.\n",
    "    \"\"\"\n",
    "    entity_types = [\n",
    "        \"PERSON\",\n",
    "        \"DATE\",\n",
    "        \"TIME\",\n",
    "        \"PERCENT\",\n",
    "        \"MONEY\",\n",
    "        \"QUANTITY\",\n",
    "        \"ORDINAL\",\n",
    "        \"CARDINAL\",\n",
    "    ]\n",
    "\n",
    "    if token.ent_type_ in entity_types:\n",
    "        return False\n",
    "\n",
    "    if token.is_punct:\n",
    "        return False\n",
    "\n",
    "    if token.is_stop:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T01:57:29.142880Z",
     "start_time": "2018-03-10T01:57:29.125567Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    sentence_stream = list()\n",
    "\n",
    "    c = 0\n",
    "\n",
    "    for doc in corpus_st:\n",
    "        c += 1\n",
    "        stdout.write(\"\\rGetting sentences from document %s/%s\" % (c, len(corpus_st)))\n",
    "\n",
    "        parsed_doc = nlp(doc, disable=[\"tagger\"])\n",
    "\n",
    "        for sentence in parsed_doc.sents:\n",
    "            sentence_tokens = list()\n",
    "            for token in sentence:\n",
    "                if not token.is_punct and not token.is_stop:\n",
    "                    sentence_tokens.append(token.lemma_.lower())\n",
    "\n",
    "            #         sentence_tokens = [token.lemma_.lower() for token in sentence if return_token(token)]\n",
    "            sentence_stream.append(sentence_tokens)\n",
    "\n",
    "    sentence_stream_directory = os.path.join(saves_directory, \"sentence-stream-st\")\n",
    "\n",
    "    with open(sentence_stream_directory, \"wb\") as fh:\n",
    "        pickle.dump(sentence_stream, fh)\n",
    "\n",
    "    print(\"\\nSaved sentence_stream list to %s\" % sentence_stream_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T11:05:59.086075Z",
     "start_time": "2018-03-10T11:05:59.078958Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentence_stream(corpus):\n",
    "    \"\"\"\n",
    "    Generator: iterate over documents in corpus_st\n",
    "\n",
    "    Argument:\n",
    "        Corpus is a list of lists of documents.\n",
    "\n",
    "    Yields:\n",
    "        List of tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    for doc in corpus:\n",
    "        #     for doc in corpus[:5000]:\n",
    "        parsed_doc = nlp(doc, disable=[\"tagger\"])\n",
    "\n",
    "        for sentence in parsed_doc.sents:\n",
    "            sentence_tokens = [\n",
    "                token.lemma_.lower() for token in sentence if return_token(token)\n",
    "            ]\n",
    "\n",
    "            yield sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:21:43.868522Z",
     "start_time": "2018-03-07T13:21:07.684382Z"
    }
   },
   "outputs": [],
   "source": [
    "# sentence_stream_directory = os.path.join(saves_directory, 'sentence-stream-st')\n",
    "\n",
    "# with open(sentence_stream_directory, 'rb') as fh:\n",
    "#     sentence_stream = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T13:20:56.725923Z",
     "start_time": "2018-03-07T13:20:56.721450Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(sentence_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T08:55:52.246523Z",
     "start_time": "2018-03-10T08:55:48.753125Z"
    }
   },
   "outputs": [],
   "source": [
    "ss = list(sentence_stream(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-10T08:55:56.324337Z",
     "start_time": "2018-03-10T08:55:56.042912Z"
    }
   },
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T04:36:33.347237Z",
     "start_time": "2018-03-11T01:14:19.296091Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_phrases = Phrases(\n",
    "    sentences=sentence_stream(corpus),\n",
    "    min_count=10,\n",
    "    threshold=0.7,\n",
    "    max_vocab_size=40000000,\n",
    "    delimiter=\"_\",\n",
    "    scoring=\"npmi\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T11:38:21.280978Z",
     "start_time": "2018-03-11T11:37:01.419824Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_phraser = Phraser(bigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T11:38:26.187548Z",
     "start_time": "2018-03-11T11:38:25.856659Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent in ss:\n",
    "    print \" \".join(bigram_phraser[sent])\n",
    "    print \"---------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T11:40:31.743452Z",
     "start_time": "2018-03-11T11:40:31.692693Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_phraser_directory = os.path.join(saves_directory, \"bigram-phraser-st\")\n",
    "bigram_phraser.save(bigram_phraser_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T11:41:21.661515Z",
     "start_time": "2018-03-11T11:41:07.379611Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_sentences = [bigram_phraser[sent] for sent in sentence_stream(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-06T14:09:30.533707Z",
     "start_time": "2018-03-06T14:06:55.976971Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_sentence_stream_directory = os.path.join(\n",
    "    saves_directory, \"bigram-sentence-stream-st\"\n",
    ")\n",
    "\n",
    "with open(bigram_sentence_stream_directory, \"wb\") as fh:\n",
    "    pickle.dump(bigram_sentences, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigram phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T14:51:23.828363Z",
     "start_time": "2018-03-11T11:41:35.578071Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(\n",
    "    sentences=bigram_phraser[sentence_stream(corpus)],\n",
    "    min_count=10,\n",
    "    threshold=0.7,\n",
    "    max_vocab_size=40000000,\n",
    "    delimiter=\"_\",\n",
    "    scoring=\"npmi\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:02:32.377649Z",
     "start_time": "2018-03-11T15:01:09.482162Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(trigram_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:05:33.510858Z",
     "start_time": "2018-03-11T15:05:33.061493Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent in bigram_phraser[ss]:\n",
    "    print \" \".join(trigram_phraser[sent])\n",
    "    print '------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:05:57.371941Z",
     "start_time": "2018-03-11T15:05:57.298316Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_phraser_directory = os.path.join(saves_directory, \"trigram-phraser-st\")\n",
    "trigram_phraser.save(trigram_phraser_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T15:13:04.718179Z",
     "start_time": "2018-03-11T15:12:55.265982Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_st_directory = os.path.join(saves_directory, \"corpus-st\")\n",
    "\n",
    "with open(corpus_st_directory, \"rb\") as fh:\n",
    "    corpus_st = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T17:15:26.843011Z",
     "start_time": "2018-03-11T15:13:19.302885Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = list()\n",
    "\n",
    "\n",
    "for ix, doc in enumerate(corpus_st):\n",
    "    stdout.write(\"\\rTokenizing document %s/%s\" % (ix + 1, len(corpus_st)))\n",
    "\n",
    "    tokenized_document = list()\n",
    "\n",
    "    # parse doc (str/unicode) using spacy's nlp\n",
    "    parsed_doc = nlp(doc, disable=[\"tagger\", \"ner\"])\n",
    "\n",
    "    # append lemma of token if not punctuation and not stopword\n",
    "    for token in parsed_doc:\n",
    "        if not token.is_punct and not token.is_stop:\n",
    "            tokenized_document.append(token.lemma_.lower())\n",
    "\n",
    "    tokenized_corpus.append(tokenized_document)\n",
    "\n",
    "\n",
    "# save\n",
    "tokenized_corpus_directory = os.path.join(saves_directory, \"tokenized_corpus-st\")\n",
    "\n",
    "with open(tokenized_corpus_directory, \"wb\") as fh:\n",
    "    pickle.dump(tokenized_corpus, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting tokenized corpus to phrases and vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-07T14:24:00.275260Z",
     "start_time": "2018-03-07T14:23:36.703339Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_corpus_directory = os.path.join(saves_directory, \"tokenized_corpus-st\")\n",
    "\n",
    "with open(tokenized_corpus_directory, \"rb\") as fh:\n",
    "    tokenized_corpus = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T17:17:57.923479Z",
     "start_time": "2018-03-11T17:16:13.038418Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_phraser_directory = os.path.join(saves_directory, \"bigram-phraser-st\")\n",
    "bigram_phraser = Phraser.load(bigram_phraser_directory)\n",
    "\n",
    "bigram_corpus = [bigram_phraser[sent] for sent in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T17:24:40.820538Z",
     "start_time": "2018-03-11T17:21:57.768137Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_phraser_directory = os.path.join(saves_directory, \"trigram-phraser-st\")\n",
    "trigram_phraser = Phraser.load(trigram_phraser_directory)\n",
    "\n",
    "trigram_corpus = [trigram_phraser[sent] for sent in bigram_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T17:26:58.145579Z",
     "start_time": "2018-03-11T17:26:12.147007Z"
    }
   },
   "outputs": [],
   "source": [
    "trigram_corpus_directory = os.path.join(saves_directory, \"trigram-corpus-st\")\n",
    "\n",
    "with open(trigram_corpus_directory, \"wb\") as f:\n",
    "    pickle.dump(trigram_corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T17:30:42.062774Z",
     "start_time": "2018-03-11T17:30:14.418996Z"
    }
   },
   "outputs": [],
   "source": [
    "# load corpus\n",
    "trigram_corpus_directory = os.path.join(saves_directory, \"trigram-corpus-st\")\n",
    "with open(trigram_corpus_directory, \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T17:32:06.757518Z",
     "start_time": "2018-03-11T17:31:44.055892Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(corpus)\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "\n",
    "dictionary_directory = os.path.join(saves_directory, \"trigram-dictionary-st.dict\")\n",
    "dictionary.save(dictionary_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T17:34:44.425444Z",
     "start_time": "2018-03-11T17:33:11.876568Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorized_corpus = [dictionary.doc2bow(doc) for doc in trigram_corpus]\n",
    "\n",
    "vectorized_corpus_directory = os.path.join(\n",
    "    saves_directory, \"trigram-vectorized-corpus-st.mm\"\n",
    ")\n",
    "MmCorpus.serialize(vectorized_corpus_directory, vectorized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T01:06:35.766320Z",
     "start_time": "2018-03-11T01:06:35.761872Z"
    }
   },
   "source": [
    "# Find optimal k\n",
    "Requires:\n",
    "* vectorized_corpus\n",
    "* dictionary\n",
    "* corpus (in text form - list of lists of document tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get coherence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:50:34.093345Z",
     "start_time": "2018-03-21T10:50:00.699403Z"
    }
   },
   "outputs": [],
   "source": [
    "# load vectorized_corpus as stream\n",
    "vectorized_corpus_directory = os.path.join(\n",
    "    saves_directory, \"trigram-vectorized-corpus-st.mm\"\n",
    ")\n",
    "vectorized_corpus = MmCorpus(vectorized_corpus_directory)\n",
    "\n",
    "# load dictionary\n",
    "dictionary_directory = os.path.join(saves_directory, \"trigram-dictionary-st.dict\")\n",
    "dictionary = Dictionary.load(dictionary_directory)\n",
    "\n",
    "# load corpus\n",
    "trigram_corpus_directory = os.path.join(saves_directory, \"trigram-corpus-st\")\n",
    "with open(trigram_corpus_directory, \"rb\") as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T10:50:42.722353Z",
     "start_time": "2018-03-21T10:50:42.704522Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    columns=[\"c_v\", \"c_uci\", \"c_npmi\", \"u_mass\", \"num_topics\"]\n",
    ").set_index(\"num_topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-15T04:12:22.004892Z",
     "start_time": "2018-03-15T04:12:21.968734Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_excel('coherence-scores-st.xlsx', index_col='num_topics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T14:19:36.676408Z",
     "start_time": "2018-03-21T10:56:11.810881Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_TOPICS = 30\n",
    "TOPN = 10  # top n words in topics to use when evaluating topic coherence\n",
    "PROCESSES = 1  # I think this is how cpu cores to use when estimating coherence\n",
    "\n",
    "for k in np.arange(2, MAX_TOPICS, 2):\n",
    "    stdout.write(\n",
    "        \"\\rTopic modelling %s topics (%s)\" % (k, time.strftime(\"%Y-%m-%d %H:%M\"))\n",
    "    )\n",
    "\n",
    "    # train base LDA model\n",
    "    tm = LdaMulticore(\n",
    "        corpus=vectorized_corpus,\n",
    "        num_topics=k,\n",
    "        id2word=dictionary,\n",
    "        workers=2,\n",
    "        chunksize=2000,\n",
    "        passes=1,\n",
    "        batch=False,\n",
    "        alpha=\"symmetric\",\n",
    "        eta=None,\n",
    "        decay=0.5,\n",
    "        offset=1.0,\n",
    "        eval_every=10,\n",
    "        iterations=100,\n",
    "        gamma_threshold=0.001,\n",
    "        random_state=0,\n",
    "        minimum_probability=0.01,\n",
    "        minimum_phi_value=0.01,\n",
    "        per_word_topics=False,\n",
    "    )\n",
    "\n",
    "    # Train coherence models\n",
    "    c_v_model = CoherenceModel(\n",
    "        model=tm,\n",
    "        topics=None,\n",
    "        texts=corpus,\n",
    "        corpus=vectorized_corpus,\n",
    "        dictionary=None,\n",
    "        coherence=\"c_v\",\n",
    "        topn=TOPN,\n",
    "        processes=PROCESSES,\n",
    "    )\n",
    "\n",
    "    c_uci_model = CoherenceModel(\n",
    "        model=tm,\n",
    "        topics=None,\n",
    "        texts=corpus,\n",
    "        corpus=vectorized_corpus,\n",
    "        dictionary=None,\n",
    "        coherence=\"c_uci\",\n",
    "        topn=TOPN,\n",
    "        processes=PROCESSES,\n",
    "    )\n",
    "\n",
    "    c_npmi_model = CoherenceModel(\n",
    "        model=tm,\n",
    "        topics=None,\n",
    "        texts=corpus,\n",
    "        corpus=vectorized_corpus,\n",
    "        dictionary=None,\n",
    "        coherence=\"c_npmi\",\n",
    "        topn=TOPN,\n",
    "        processes=PROCESSES,\n",
    "    )\n",
    "\n",
    "    u_mass_model = CoherenceModel(\n",
    "        model=tm,\n",
    "        topics=None,\n",
    "        texts=corpus,\n",
    "        corpus=vectorized_corpus,\n",
    "        dictionary=None,\n",
    "        coherence=\"u_mass\",\n",
    "        topn=TOPN,\n",
    "        processes=PROCESSES,\n",
    "    )\n",
    "\n",
    "    # store coherence scores\n",
    "    df2.set_value(k, \"c_v\", c_v_model.get_coherence())\n",
    "    df2.set_value(k, \"c_uci\", c_uci_model.get_coherence())\n",
    "    df2.set_value(k, \"c_npmi\", c_npmi_model.get_coherence())\n",
    "    df2.set_value(k, \"u_mass\", u_mass_model.get_coherence())\n",
    "#     df2.to_excel('coherence-scores-st.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T17:23:13.949805Z",
     "start_time": "2018-03-23T17:23:13.847693Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"coherence-scores-st.xlsx\", index_col=\"num_topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T17:23:14.064866Z",
     "start_time": "2018-03-23T17:23:14.057474Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.apply(lambda c: pd.to_numeric(c, errors=\"coerce\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T17:23:14.907107Z",
     "start_time": "2018-03-23T17:23:14.889999Z"
    }
   },
   "outputs": [],
   "source": [
    "glob_params = {\n",
    "    \"legend.fontsize\": \"xx-large\",\n",
    "    \"figure.titlesize\": \"xx-large\",\n",
    "    \"axes.labelsize\": \"xx-large\",\n",
    "    \"axes.titlesize\": \"xx-large\",\n",
    "    \"xtick.labelsize\": \"x-large\",\n",
    "    \"ytick.labelsize\": \"x-large\",\n",
    "    \"lines.markersize\": 12.0,\n",
    "    \"figure.figsize\": [12, 8],\n",
    "}\n",
    "\n",
    "plt.rcParams.update(glob_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T17:23:18.163206Z",
     "start_time": "2018-03-23T17:23:18.146899Z"
    }
   },
   "outputs": [],
   "source": [
    "figures_directory = os.path.join(\"\", os.path.join(\"figures\", \"st\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T17:23:18.444227Z",
     "start_time": "2018-03-23T17:23:18.440785Z"
    }
   },
   "outputs": [],
   "source": [
    "check_create_directory(figures_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot c_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:48:10.628403Z",
     "start_time": "2018-03-14T21:48:10.533209Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"c_v\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:54:19.260637Z",
     "start_time": "2018-03-14T21:54:18.859566Z"
    }
   },
   "outputs": [],
   "source": [
    "c_v_directory = os.path.join(figures_directory, \"c-v-article\")\n",
    "\n",
    "local_max = (df[\"c_v\"].argmax(), df[\"c_v\"].max())\n",
    "\n",
    "plt.plot(df.index, df[\"c_v\"])\n",
    "\n",
    "plt.plot(local_max[0], local_max[1], \"o\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# https://matplotlib.org/users/annotations.html\n",
    "plt.annotate(\n",
    "    \"Local max\\n (\" + str(local_max[0]) + \", \" + str(local_max[1])[:5] + \")\",\n",
    "    xy=(local_max),\n",
    "    xytext=(local_max[0] + 5, local_max[1] - 0.01),\n",
    "    size=\"x-large\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"c_v coherence measure\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(c_v_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot c_uci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:51:28.616972Z",
     "start_time": "2018-03-14T21:51:28.608272Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"c_uci\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:54:09.224108Z",
     "start_time": "2018-03-14T21:54:08.814845Z"
    }
   },
   "outputs": [],
   "source": [
    "c_uci_directory = os.path.join(figures_directory, \"c-uci-article\")\n",
    "\n",
    "local_max = (df[\"c_uci\"].argmax(), df[\"c_uci\"].max())\n",
    "\n",
    "plt.plot(df.index, df[\"c_uci\"])\n",
    "\n",
    "plt.plot(local_max[0], local_max[1], \"o\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# https://matplotlib.org/users/annotations.html\n",
    "plt.annotate(\n",
    "    \"Local max\\n (\" + str(local_max[0]) + \", \" + str(local_max[1])[:5] + \")\",\n",
    "    xy=(local_max),\n",
    "    xytext=(local_max[0] + 5, local_max[1] - 0.02),\n",
    "    size=\"x-large\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"c_uci coherence measure\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(c_uci_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot c_npmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:51:36.548977Z",
     "start_time": "2018-03-14T21:51:36.537273Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"c_npmi\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:53:59.905956Z",
     "start_time": "2018-03-14T21:53:59.562173Z"
    }
   },
   "outputs": [],
   "source": [
    "c_npmi_directory = os.path.join(figures_directory, \"c-npmi-article\")\n",
    "\n",
    "local_max = (df[\"c_npmi\"].argmax(), df[\"c_npmi\"].max())\n",
    "\n",
    "plt.plot(df.index, df[\"c_npmi\"])\n",
    "\n",
    "plt.plot(local_max[0], local_max[1], \"o\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# https://matplotlib.org/users/annotations.html\n",
    "plt.annotate(\n",
    "    \"Local max\\n (\" + str(local_max[0]) + \", \" + str(local_max[1])[:5] + \")\",\n",
    "    xy=(local_max),\n",
    "    xytext=(local_max[0] + 5, local_max[1] - 0.005),\n",
    "    size=\"x-large\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"c_npmi coherence measure\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(c_npmi_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot u_mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:51:40.934295Z",
     "start_time": "2018-03-14T21:51:40.909170Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"u_mass\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:51:46.388117Z",
     "start_time": "2018-03-14T21:51:45.941155Z"
    }
   },
   "outputs": [],
   "source": [
    "u_mass_directory = os.path.join(figures_directory, \"u-mass-article\")\n",
    "\n",
    "local_max = (df[\"u_mass\"].argmax(), df[\"u_mass\"].max())\n",
    "\n",
    "plt.plot(df.index, df[\"u_mass\"])\n",
    "\n",
    "plt.plot(local_max[0], local_max[1], \"o\", color=\"red\", alpha=0.7)\n",
    "\n",
    "# https://matplotlib.org/users/annotations.html\n",
    "plt.annotate(\n",
    "    \"local max\\n (\" + str(local_max[0]) + \", \" + str(local_max[1])[:5] + \")\",\n",
    "    xy=(local_max),\n",
    "    xytext=(round(local_max[0], -1), round(local_max[1], 1)),\n",
    "    arrowprops=dict(facecolor=\"black\"),\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"u_mass coherence measure\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(u_mass_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot normalised scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T17:23:40.076675Z",
     "start_time": "2018-03-23T17:23:40.006192Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalising coherence scores\n",
    "def normalise_scores(vector):\n",
    "    \"\"\"\n",
    "    Normalise scores to (0, 1) range using:\n",
    "        x_normalised = (x - x_min)/(x_max - x_min)\n",
    "\n",
    "    Argument:\n",
    "        List, array, series type\n",
    "\n",
    "    Returns:\n",
    "        Transformed list, array, series\n",
    "    \"\"\"\n",
    "\n",
    "    # cach min and max\n",
    "    min_x = vector.min()\n",
    "    range_x = vector.max() - min_x\n",
    "\n",
    "    transformed_vector = [(x - min_x) / range_x for x in vector]\n",
    "\n",
    "    return transformed_vector\n",
    "\n",
    "\n",
    "transformed_scores_df = df.apply(normalise_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:56:35.009389Z",
     "start_time": "2018-03-14T21:56:34.990180Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"c_npmi\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:56:35.022114Z",
     "start_time": "2018-03-14T21:56:35.012119Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"c_v\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:56:35.033598Z",
     "start_time": "2018-03-14T21:56:35.024437Z"
    }
   },
   "outputs": [],
   "source": [
    "df.nlargest(5, \"c_uci\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:55:52.702683Z",
     "start_time": "2018-03-14T21:55:52.685019Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_scores_df.nlargest(5, \"c_npmi\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:56:10.442134Z",
     "start_time": "2018-03-14T21:56:10.427246Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_scores_df.nlargest(5, \"c_v\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T21:56:10.442134Z",
     "start_time": "2018-03-14T21:56:10.427246Z"
    }
   },
   "outputs": [],
   "source": [
    "transformed_scores_df.nlargest(5, \"c_uci\").index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-23T17:23:50.369742Z",
     "start_time": "2018-03-23T17:23:49.906287Z"
    }
   },
   "outputs": [],
   "source": [
    "normalised_directory = os.path.join(figures_directory, \"normalised-scores-article\")\n",
    "\n",
    "local_max = (\n",
    "    transformed_scores_df[\"c_npmi\"].argmax(),\n",
    "    transformed_scores_df[\"c_npmi\"].max(),\n",
    ")\n",
    "\n",
    "# create plot\n",
    "plt.plot(transformed_scores_df.index, transformed_scores_df[\"c_v\"], color=\"teal\")\n",
    "plt.plot(transformed_scores_df.index, transformed_scores_df[\"c_uci\"], color=\"orange\")\n",
    "plt.plot(transformed_scores_df.index, transformed_scores_df[\"c_npmi\"], color=\"maroon\")\n",
    "\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Normalised coherence scores [0, 1]\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.savefig(normalised_directory)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T22:01:33.025023Z",
     "start_time": "2018-03-14T22:01:32.980745Z"
    }
   },
   "outputs": [],
   "source": [
    "dictionary_directory = os.path.join(saves_directory, \"trigram-dictionary-st.dict\")\n",
    "dictionary = Dictionary.load(dictionary_directory)\n",
    "\n",
    "vectorized_corpus_directory = os.path.join(\n",
    "    saves_directory, \"trigram-vectorized-corpus-st.mm\"\n",
    ")\n",
    "vectorized_corpus = MmCorpus(vectorized_corpus_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K*=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T22:14:39.278159Z",
     "start_time": "2018-03-14T22:01:39.824632Z"
    }
   },
   "outputs": [],
   "source": [
    "lda40 = LdaMulticore(\n",
    "    corpus=vectorized_corpus,\n",
    "    num_topics=40,\n",
    "    id2word=dictionary,\n",
    "    chunksize=2000,\n",
    "    passes=1,\n",
    "    workers=2,\n",
    "    alpha=\"symmetric\",\n",
    "    eta=None,\n",
    "    decay=0.5,\n",
    "    offset=1.0,\n",
    "    eval_every=None,\n",
    "    iterations=200,\n",
    "    gamma_threshold=0.001,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=0,\n",
    "    minimum_phi_value=0.01,\n",
    "    per_word_topics=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T22:24:05.461597Z",
     "start_time": "2018-03-14T22:24:05.388229Z"
    }
   },
   "outputs": [],
   "source": [
    "lda40_directory = os.path.join(saves_directory, \"lda-st-40.model\")\n",
    "lda40.save(lda40_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T22:31:57.706319Z",
     "start_time": "2018-03-14T22:24:11.322141Z"
    }
   },
   "outputs": [],
   "source": [
    "lda30 = LdaMulticore(\n",
    "    corpus=vectorized_corpus,\n",
    "    num_topics=30,\n",
    "    id2word=dictionary,\n",
    "    chunksize=2000,\n",
    "    passes=1,\n",
    "    workers=2,\n",
    "    alpha=\"symmetric\",\n",
    "    eta=None,\n",
    "    decay=0.5,\n",
    "    offset=1.0,\n",
    "    eval_every=None,\n",
    "    iterations=200,\n",
    "    gamma_threshold=0.001,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=0,\n",
    "    minimum_phi_value=0.01,\n",
    "    per_word_topics=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T22:52:57.434170Z",
     "start_time": "2018-03-14T22:52:57.373975Z"
    }
   },
   "outputs": [],
   "source": [
    "lda30_directory = os.path.join(saves_directory, \"lda-st-30.model\")\n",
    "lda30.save(lda30_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T23:06:35.018667Z",
     "start_time": "2018-03-14T22:53:00.816299Z"
    }
   },
   "outputs": [],
   "source": [
    "lda50 = LdaMulticore(\n",
    "    corpus=vectorized_corpus,\n",
    "    num_topics=50,\n",
    "    id2word=dictionary,\n",
    "    chunksize=2000,\n",
    "    passes=1,\n",
    "    workers=2,\n",
    "    alpha=\"symmetric\",\n",
    "    eta=None,\n",
    "    decay=0.5,\n",
    "    offset=1.0,\n",
    "    eval_every=None,\n",
    "    iterations=200,\n",
    "    gamma_threshold=0.001,\n",
    "    minimum_probability=0.01,\n",
    "    random_state=0,\n",
    "    minimum_phi_value=0.01,\n",
    "    per_word_topics=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-14T23:15:35.595923Z",
     "start_time": "2018-03-14T23:15:35.504299Z"
    }
   },
   "outputs": [],
   "source": [
    "lda50_directory = os.path.join(saves_directory, \"lda-st-50.model\")\n",
    "lda50.save(lda50_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_media)",
   "language": "python",
   "name": "venv_media"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "464.92308px",
    "left": "0.201923px",
    "right": "20px",
    "top": "479.962px",
    "width": "113.92308px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
